{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reedmarkham/meter-made/blob/main/meter_made.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library imports, etc."
      ],
      "metadata": {
        "id": "By6CeD1WSpW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import itertools\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pyproj\n",
        "from shapely.geometry import Point\n",
        "from shapely.ops import transform\n",
        "import geopandas as gpd\n",
        "import folium\n",
        "import branca.colormap as cm\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "import lightgbm as lgb"
      ],
      "metadata": {
        "id": "_OrwoZcSa9j2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandarallel\n",
        "from pandarallel import pandarallel\n",
        "pandarallel.initialize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-4ODymiemFf",
        "outputId": "7eaff262-2705-4441-d7a7-6a5b9997f4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandarallel in /usr/local/lib/python3.11/dist-packages (1.6.5)\n",
            "Requirement already satisfied: dill>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from pandarallel) (0.3.9)\n",
            "Requirement already satisfied: pandas>=1 in /usr/local/lib/python3.11/dist-packages (from pandarallel) (2.2.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from pandarallel) (5.9.5)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->pandarallel) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->pandarallel) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->pandarallel) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1->pandarallel) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1->pandarallel) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzbA9gmnVLTN"
      },
      "source": [
        "# Load the DC parking data previously downloaded to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGlYRV9OsQkU"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "!ls '/content/drive/MyDrive/dc-parking-data'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check out the DC street sweeping routes"
      ],
      "metadata": {
        "id": "0AZ-r8N7oOwD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyN8R7y1VlyA"
      },
      "outputs": [],
      "source": [
        "zones = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Zones.geojson')\n",
        "ax = zones[zones.PARKINGGROUP == 'Sweeping'].plot(column='ROUTEID')\n",
        "ax.set_axis_off()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest addresses and routes"
      ],
      "metadata": {
        "id": "jhK7I6H3bJH3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "addresses = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Address_Points.geojson')\n",
        "a_cols = ['ADDRESS', 'ZIPCODE', 'BLOCKKEY', 'ROUTEID', 'LATITUDE', 'LONGITUDE']\n",
        "addresses = addresses[a_cols]\n",
        "\n",
        "# & (gdf.ROUTEID == '11072862')\n",
        "routes = zones[(zones.PARKINGGROUP == 'Sweeping') & (zones.PARKINGDAYGROUP == 'Monday') & (zones.PARKINGHOURGROUP == 'AM')].set_crs('EPSG:4326')\n",
        "del zones\n",
        "\n",
        "routes = routes.sort_values(by='MEAS_FROM')\n",
        "r_cols = ['ROUTEID', 'ZONEID', 'SIGNS', 'SIGNTEXT', 'SIGNCODE', 'MEAS_FROM', 'MEAS_TO', 'STARTTIME', 'ENDTIME', 'BLOCKFACEKEY', 'geometry']\n",
        "routes = routes[r_cols]\n",
        "routes_df = routes.merge(addresses, on='ROUTEID', how='inner')\n",
        "del routes\n",
        "\n",
        "routes_gdf = gpd.GeoDataFrame(\n",
        "    routes_df,\n",
        "    geometry=gpd.points_from_xy(routes_df.LONGITUDE, routes_df.LATITUDE),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "del routes_df\n",
        "\n",
        "display(routes_gdf.sort_values(by='ADDRESS'))\n",
        "del routes_gdf"
      ],
      "metadata": {
        "id": "XF_k2UGsS64b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aFF5bvM2LLsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ingest tickets"
      ],
      "metadata": {
        "id": "O720-A3UbHJT"
      }
    },
    {
      "source": [
        "t_jan = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_January_2024.geojson')\n",
        "t_feb = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_February_2024.geojson')\n",
        "t_mar = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_March_2024.geojson')\n",
        "t_apr = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_April_2024.geojson')\n",
        "t_may = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_May_2024.geojson')\n",
        "t_jun = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_June_2024.geojson')\n",
        "t_jul = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_July_2024.geojson')\n",
        "t_aug = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_August_2024.geojson')\n",
        "t_sep = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_September_2024.geojson')\n",
        "t_oct = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_October_2024.geojson')\n",
        "t_nov = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_November_2024.geojson')\n",
        "t_dec = gpd.read_file('/content/drive/MyDrive/dc-parking-data/Parking_Violations_Issued_in_December_2024.geojson')\n",
        "t = pd.concat([t_jan, t_feb, t_mar, t_apr, t_may, t_jun, t_jul, t_aug, t_sep, t_oct, t_nov, t_dec])\n",
        "\n",
        "for t_ in [t_jan, t_feb, t_mar, t_apr, t_may, t_jun, t_jul, t_aug, t_sep, t_oct, t_nov, t_dec]:\n",
        "  del t_\n",
        "\n",
        "tickets = gpd.GeoDataFrame(\n",
        "    t,\n",
        "    geometry=gpd.points_from_xy(t.LONGITUDE, t.LATITUDE),\n",
        "    crs=\"EPSG:4326\"\n",
        ")\n",
        "\n",
        "# Fill NaN values in VIOLATION_PROC_DESC with an empty string before using .str.contains()\n",
        "tickets['VIOLATION_PROC_DESC'] = tickets['VIOLATION_PROC_DESC'].fillna('')\n",
        "display(tickets[tickets.VIOLATION_PROC_DESC.str.contains('EXPIRED METER')])"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "S_pPwlbIxKio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Join street cleaning parking tickets with sweeping routes"
      ],
      "metadata": {
        "id": "Ty81DfbEnbqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "routes_gdf.geometry = routes_gdf.geometry.set_precision(grid_size=0.0001).force_2d()\n",
        "cleaning_tix = tickets[tickets.VIOLATION_CODE == 'P173']\n",
        "cleaning_tix.geometry = cleaning_tix.geometry.set_precision(grid_size=0.0001).force_2d()\n",
        "tickets_and_routes = gpd.sjoin(routes_gdf, cleaning_tix, how='inner', predicate='intersects')\n",
        "\n",
        "del routes_gdf\n",
        "del cleaning_tix\n",
        "\n",
        "tickets_and_routes = tickets_and_routes[['TICKET_NUMBER', 'ROUTEID', 'ISSUE_DATE', 'ISSUE_TIME', 'geometry']]\n",
        "tickets_and_routes = tickets_and_routes.sort_values(by='TICKET_NUMBER').drop_duplicates()\n",
        "tickets_and_routes.geometry = tickets_and_routes.geometry.set_precision(grid_size=0.00001)\n",
        "display(tickets_and_routes)"
      ],
      "metadata": {
        "id": "yrDSOzMzWBjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tickets_and_routes['timestamp'] = tickets_and_routes.apply(\n",
        "    lambda row: str(row['ISSUE_DATE'].date()) + ' ' + row['ISSUE_TIME'],\n",
        "    axis=1\n",
        "    )\n",
        "tickets_and_routes['timestamp'] = pd.to_datetime(\n",
        "    tickets_and_routes['timestamp'],\n",
        "    format='%Y-%m-%d %H%M',\n",
        "    errors='coerce'\n",
        "    )\n",
        "tickets_and_routes = tickets_and_routes[['geometry', 'ROUTEID', 'timestamp']]\n",
        "display(tickets_and_routes)"
      ],
      "metadata": {
        "id": "bhhbyJ-VkaIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualize frequency and order (by ticket timestamp) of parking tickets along routes"
      ],
      "metadata": {
        "id": "X2Rl5gSznlr0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DC_COORDS = [38.90720, -77.03690]\n",
        "\n",
        "tickets_and_routes_filtered = tickets_and_routes[\n",
        "    tickets_and_routes['timestamp'].dt.date == pd.to_datetime('2024-10-28').date()\n",
        "]\n",
        "\n",
        "del tickets_and_routes\n",
        "\n",
        "map = folium.Map(location=DC_COORDS, zoom_start=12, tiles=\"cartodb positron\")\n",
        "\n",
        "#TO-DO: compute elapsed_minutes within ROUTEID as well as colormap but then plot for each ROUTEID layer the respective colormap and elapsed_minutes in popup (\"since street cleaning route started\")\n",
        "\n",
        "tickets_and_routes_filtered['elapsed_minutes'] = (tickets_and_routes_filtered['timestamp'] - tickets_and_routes_filtered['timestamp'].min()).dt.total_seconds() / 60\n",
        "\n",
        "colormap = cm.linear.Blues_03.scale(\n",
        "    tickets_and_routes_filtered.elapsed_minutes.min(),\n",
        "    tickets_and_routes_filtered.elapsed_minutes.max()\n",
        "    )\n",
        "\n",
        "geometry_counts = tickets_and_routes_filtered.groupby(['geometry', 'ROUTEID'])['timestamp'].count().reset_index()\n",
        "earliest_timestamps = tickets_and_routes_filtered.groupby(['geometry', 'ROUTEID'])['timestamp'].min().rename('earliest_timestamp').reset_index()\n",
        "\n",
        "route_layers = {}\n",
        "for route_id in tickets_and_routes_filtered['ROUTEID'].unique():\n",
        "  route_data = tickets_and_routes_filtered[tickets_and_routes_filtered['ROUTEID'] == route_id]\n",
        "  route_fg = folium.FeatureGroup(name=f\"Route {route_id}\")\n",
        "\n",
        "  for idx, row in route_data.sort_values(by='elapsed_minutes').iterrows():\n",
        "      lon = row.geometry.x\n",
        "      lat = row.geometry.y\n",
        "      count_for_geometry = int(geometry_counts[geometry_counts.geometry == row.geometry]['timestamp'].iloc[0] if geometry_counts[geometry_counts.geometry == row.geometry]['timestamp'].size > 0 else 0)\n",
        "      earliest_timestamp = earliest_timestamps[(earliest_timestamps.geometry == row.geometry) & (earliest_timestamps.ROUTEID == route_id)]['earliest_timestamp'].iloc[0] if earliest_timestamps[(earliest_timestamps.geometry == row.geometry) & (earliest_timestamps.ROUTEID == route_id)]['earliest_timestamp'].size > 0 else None\n",
        "\n",
        "      popup_html = f\"\"\"\n",
        "      <h1> Street sweeping summary:</h1>\n",
        "      <p># of tickets: {count_for_geometry}</p>\n",
        "      <p>Started at: {earliest_timestamp}</p>\n",
        "      \"\"\"\n",
        "\n",
        "      folium.CircleMarker(\n",
        "          location=[lat, lon],\n",
        "          radius=count_for_geometry,\n",
        "          color='none',\n",
        "          fill=True,\n",
        "          fill_color=colormap(row.elapsed_minutes),\n",
        "          fill_opacity=0.7,\n",
        "          popup=popup_html\n",
        "      ).add_to(route_fg)\n",
        "\n",
        "  route_layers[route_id] = route_fg\n",
        "  route_fg.add_to(map)\n",
        "\n",
        "del route_layers\n",
        "del tickets_and_routes_filtered\n",
        "del geometry_counts\n",
        "del earliest_timestamps\n",
        "\n",
        "colormap.add_to(map)\n",
        "folium.LayerControl().add_to(map)\n",
        "\n",
        "map\n",
        "\n",
        "del map"
      ],
      "metadata": {
        "id": "uMtlL9y-sr0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compute clusters of expired parking meter ticket locations (x, y coords) using DBSCAN"
      ],
      "metadata": {
        "id": "WY87yRFYn2yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tix = tickets[tickets.VIOLATION_CODE == 'P039']\n",
        "\n",
        "array_data = []\n",
        "for index, row in tix.iterrows():\n",
        "  if row.geometry and row.geometry.is_valid and not row.geometry.is_empty:\n",
        "    x_coord = row.geometry.x\n",
        "    y_coord = row.geometry.y\n",
        "    date = row['ISSUE_DATE'].date()\n",
        "\n",
        "    try:\n",
        "        time_of_day = datetime.time(int(row['ISSUE_TIME'].zfill(4)[:2]), int(row['ISSUE_TIME'].zfill(4)[2:]))\n",
        "    except (ValueError, TypeError):\n",
        "        print(f\"Invalid time format: {row['ISSUE_TIME']}, skipping row\")\n",
        "        continue\n",
        "\n",
        "    #day_of_week =  row['ISSUE_DATE'].date().isoweekday()\n",
        "    array_data.append([x_coord, y_coord, date, time_of_day])\n",
        "\n",
        "del tix\n",
        "\n",
        "X_full = np.array(array_data)\n",
        "del array_data\n",
        "\n",
        "X_full_df = pd.DataFrame(X_full, columns=['x', 'y', 'd', 't'])\n",
        "X_full_df['h'] = X_full_df['t'].apply(lambda t: (t.hour + (t.minute + 30) // 60) % 24)\n",
        "X_full = X_full_df.values"
      ],
      "metadata": {
        "id": "4_pcIs9SqF1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "to_cluster = pd.DataFrame(X_full, columns=['x', 'y', 'd', 't', 'h'])\n",
        "del X_full"
      ],
      "metadata": {
        "id": "9sqw-0PJhMRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_dbscan_clusters(to_cluster, d, h):\n",
        "  \"\"\"Computes DBSCAN clusters for a given date and hour.\n",
        "\n",
        "  Args:\n",
        "    to_cluster: A pandas DataFrame with columns 'x', 'y', 'd', 't', 'h'.\n",
        "    d: The date to filter for.\n",
        "    h: The hour to filter for.\n",
        "\n",
        "  Returns:\n",
        "    A pandas DataFrame with columns 'x', 'y', 'cluster', containing the clustered\n",
        "    data for the specified date and hour.\n",
        "  \"\"\"\n",
        "  filtered_data = to_cluster[(to_cluster['d'] == d) & (to_cluster['h'] == h)][['x', 'y']]\n",
        "  if filtered_data.empty:\n",
        "    return pd.DataFrame(columns=['x', 'y', 'cluster'])\n",
        "  db = DBSCAN(eps=0.01, min_samples=5).fit(filtered_data)\n",
        "  labels = db.labels_\n",
        "  core = db.core_sample_indices_\n",
        "  clustered_data = filtered_data.copy()\n",
        "  clustered_data['cluster'] = labels\n",
        "  return clustered_data\n",
        "\n",
        "compute_dbscan_clusters(to_cluster, datetime.date(2024, 6, 25), 11)"
      ],
      "metadata": {
        "id": "YRPvKpJ3hd9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DC_COORDS = [38.90720, -77.03690]\n",
        "map = folium.Map(location=DC_COORDS, zoom_start=12, tiles=\"cartodb positron\")\n",
        "\n",
        "date_to_plot = datetime.date(2024, 6, 25)\n",
        "hour_to_plot = 11\n",
        "\n",
        "clustered_data = compute_dbscan_clusters(to_cluster, date_to_plot, hour_to_plot)\n",
        "colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'lightred', 'beige']\n",
        "\n",
        "if not clustered_data.empty:\n",
        "  for (cluster_id, color_) in zip(clustered_data['cluster'].unique(), colors):\n",
        "    if cluster_id == -1:  # Noise points\n",
        "      continue\n",
        "\n",
        "    cluster_points = clustered_data[clustered_data['cluster'] == cluster_id]\n",
        "    if not cluster_points.empty:\n",
        "      for _, cp in cluster_points.iterrows():\n",
        "        folium.CircleMarker(\n",
        "            location=[cp['y'], cp['x']],\n",
        "            radius=3,\n",
        "            color=color_,\n",
        "            fill=True,\n",
        "            fill_color=color_,\n",
        "            fill_opacity=0.7,\n",
        "            popup=f\"Cluster {cluster_id}\"\n",
        "        ).add_to(map)\n",
        "\n",
        "map\n",
        "\n",
        "del map"
      ],
      "metadata": {
        "id": "n7YZbjZjiAag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "combinations = list(itertools.product(X_full_df['d'].unique(), X_full_df['h'].unique()))\n",
        "cluster_labels_list = []\n",
        "for d, h in combinations:\n",
        "  cluster_labels = compute_dbscan_clusters(X_full_df, d, h)\n",
        "  if not cluster_labels.empty:\n",
        "    cluster_labels['d'] = d\n",
        "    cluster_labels['h'] = h\n",
        "    cluster_labels_list.append(cluster_labels)\n",
        "\n",
        "cluster_labels_df = pd.concat(cluster_labels_list, ignore_index=True)\n",
        "\n",
        "del cluster_labels_list"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "B_ktxUjmqbdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(cluster_labels_df)"
      ],
      "metadata": {
        "id": "nUpZ6brvq0vH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict parking tickets from day, time, and cluster locations using heuristics"
      ],
      "metadata": {
        "id": "YjpDsYPWHVrw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df = cluster_labels_df[(cluster_labels_df['d'] == pd.to_datetime('2024-06-25').date()) & (cluster_labels_df['h'] == 11)]\n",
        "\n",
        "test_clusters_gdf = gpd.GeoDataFrame(\n",
        "    filtered_df,\n",
        "    geometry=gpd.points_from_xy(filtered_df.x, filtered_df.y),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "test_clusters_gdf = test_clusters_gdf.to_crs('EPSG:32618')\n",
        "test_clusters_gdf.plot()"
      ],
      "metadata": {
        "id": "WEuzKVVybFFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "addresses_gdf = gpd.GeoDataFrame(\n",
        "    addresses,\n",
        "    geometry=gpd.points_from_xy(addresses.LONGITUDE, addresses.LATITUDE),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "\n",
        "print(addresses_gdf.estimate_utm_crs())\n",
        "\n",
        "addresses_gdf = addresses_gdf.to_crs('EPSG:32618')\n",
        "display(addresses_gdf)"
      ],
      "metadata": {
        "id": "cU1TdHLQI_49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a_gdf = addresses_gdf[['ADDRESS', 'geometry']]\n",
        "del addresses_gdf\n",
        "\n",
        "a_gdf['min_dist'] = a_gdf.parallel_apply(lambda row: row.geometry.distance(test_clusters_gdf.geometry).min()/1609.34, axis=1)\n",
        "display(a_gdf)"
      ],
      "metadata": {
        "id": "DQ91aRK2btwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_ticket(dist):\n",
        "  return 1 if dist < 0.25 else 0\n",
        "\n",
        "a_gdf['ticketed'] = a_gdf['min_dist'].parallel_apply(estimate_ticket)\n",
        "a_gdf.plot(column='ticketed', legend=True)"
      ],
      "metadata": {
        "id": "b7E-2Huqf_yL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del a_gdf"
      ],
      "metadata": {
        "id": "y9fYUSPCKQY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict parking tickets from cluster locations using LightGBM"
      ],
      "metadata": {
        "id": "mCmK7yEDOxDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = cluster_labels_df[cluster_labels_df.cluster != -1]\n",
        "final_df = final_df.groupby(['d', 'h', 'cluster'])[['x', 'y']].agg(lambda x: list(zip(x.values)))\n",
        "final_df = final_df.applymap(lambda x: np.mean(np.array(x), axis=0) if x else np.array([])).reset_index()\n",
        "#final_df['coords_array'] = final_df.apply(lambda row: (float(row['x']), float(row['y'])), axis=1)\n",
        "#final_df = final_df[['d', 'h', 'cluster', 'coords_array']]\n",
        "final_df = final_df[['d', 'h', 'cluster', 'x', 'y']]\n",
        "final_df.x = final_df.x.astype('float')\n",
        "final_df.y = final_df.y.astype('float')\n",
        "display(final_df)"
      ],
      "metadata": {
        "id": "eWvO5rBIqDWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = addresses[['LONGITUDE', 'LATITUDE']]\n",
        "a['cluster'] = -1\n",
        "a['x'] = a['LONGITUDE']\n",
        "a['y'] = a['LATITUDE']\n",
        "a = a[['x', 'y', 'cluster']]\n",
        "\n",
        "display(a)"
      ],
      "metadata": {
        "id": "4BAd7IROstBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dask.dataframe as dd\n",
        "\n",
        "def cross_join_dask(df1, df2):\n",
        "    ddf1 = dd.from_pandas(df1, npartitions=4)\n",
        "    ddf2 = dd.from_pandas(df2, npartitions=4)\n",
        "    ddf1['_temp_key'] = 1\n",
        "    ddf2['_temp_key'] = 1\n",
        "    result = dd.merge(ddf1, ddf2, on='_temp_key').drop('_temp_key', axis=1).compute()\n",
        "    del ddf1\n",
        "    del ddf2\n",
        "    return result\n",
        "\n",
        "propagated_df = pd.DataFrame()\n",
        "batch_size = 30\n",
        "for i in range(0, len(a), batch_size):\n",
        "  batch = a[i:i + batch_size]\n",
        "  cross_joined_df = cross_join_dask(batch, final_df[['d', 'h']])\n",
        "  propagated_df = pd.concat([propagated_df, cross_joined_df], ignore_index=True)\n",
        "  del batch\n",
        "  del cross_joined_df\n",
        "\n",
        "del a\n",
        "display(propagated_df)"
      ],
      "metadata": {
        "id": "z8jjaER5wL6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.concat([cluster_labels_df[['x', 'y', 'cluster']], propagated_df])\n",
        "del cluster_labels_df\n",
        "display(final_df)"
      ],
      "metadata": {
        "id": "s3VgtvS7tY_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = final_df[['d','h','x', 'y']]\n",
        "y = final_df['cluster']\n",
        "X['d'] = pd.to_numeric(X['d'])\n",
        "X['h'] = pd.to_numeric(X['h'])\n",
        "X['x'] = pd.to_numeric(X['x'])\n",
        "X['y'] = pd.to_numeric(X['y'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model = lgb.LGBMClassifier(num_leaves=15)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1 Score:\", f1)"
      ],
      "metadata": {
        "id": "dCOMgvdLujNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_df = pd.DataFrame(\n",
        "    {'x': X_test['x'],\n",
        "    'y': X_test['y'],\n",
        "    'predicted_cluster': y_pred}\n",
        "    )\n",
        "\n",
        "predicted_gdf = gpd.GeoDataFrame(\n",
        "    predicted_df,\n",
        "    geometry=gpd.points_from_xy(predicted_df.x, predicted_df.y),\n",
        "    crs='EPSG:4326'\n",
        ")\n",
        "del predicted_df\n",
        "\n",
        "predicted_gdf.plot(column='predicted_cluster', legend=True)\n",
        "del predicted_gdf"
      ],
      "metadata": {
        "id": "bChYVpa4si2f"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyONORMIbtZ2Jx8iERQfETI0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}